{
  "spans": [
    {
      "name": "call_llm o3",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619768396255068,
      "end_time": 1755619773839757802,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 645916584152681710,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "call_llm",
        "gen_ai.request.model": "o3",
        "gen_ai.input.messages": "[{\"role\": \"system\", \"content\": \"\\nYou are an assistant that produces concise, accurate summaries of webpages by following this fixed, step-by-step workflow:\\n\\nSTEP-1 \u2013 Fetch webpage:\\n\u2022 Receive an input URL from the user.\\n\u2022 Call the `visit_webpage` tool to download the page as Markdown.\\n\u2022 If the tool returns an error or empty content, stop the workflow and output an appropriate message in the `summary` field.\\n\\nSTEP-2 \u2013 Extract plain text:\\n\u2022 Pass the Markdown returned in STEP-1 to `extract_text_from_markdown_or_html`, with `content_type` set to \\\"md\\\", to strip HTML/Markdown formatting and non-text elements.\\n\u2022 Trim the result to at most ~3 000 tokens (about 12 000 characters) while keeping the most informative parts.\\n\\nSTEP-3 \u2013 Summarize:\\n\u2022 Use `summarize_text_with_llm` with `summary_length=\\\"a concise paragraph\\\"` to create a clear, self-contained summary that captures the main ideas, purpose, and key details of the page.\\n\u2022 The summary must not hallucinate information or include content absent from the source.\\n\\nSTEP-4 \u2013 Final structured output:\\nReturn a JSON object that matches the `StructuredOutput` schema with these fields:\\n  \u2022 url \u2013 the original input URL.\\n  \u2022 extracted_text \u2013 the trimmed plain-text content from STEP-2.\\n  \u2022 summary \u2013 the paragraph-length summary from STEP-3 (or an error message if earlier steps failed).\\n\\nGeneral rules:\\n\u2022 Follow the steps in order and do not skip any.\\n\u2022 Use only the provided tools.\\n\u2022 Keep the language of the summary the same as the user\u2019s request (default English).\\n\u2022 Do not expose internal reasoning or tool outputs except as specified in the schema.\\n\"}, {\"content\": \"Summarize the main text content of this webpage: https://blog.mozilla.ai/introducing-any-llm-a-unified-api-to-access-any-llm-provider/\", \"role\": \"user\"}]",
        "gen_ai.output": "[{\"tool.name\": \"visit_webpage\", \"tool.args\": \"{\\\"url\\\":\\\"https://blog.mozilla.ai/introducing-any-llm-a-unified-api-to-access-any-llm-provider/\\\",\\\"timeout\\\":30,\\\"max_length\\\":20000}\"}]",
        "gen_ai.output.type": "json",
        "gen_ai.usage.input_tokens": 857,
        "gen_ai.usage.output_tokens": 57,
        "gen_ai.usage.input_cost": 0.001714,
        "gen_ai.usage.output_cost": 0.00045599999999999997
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "execute_tool visit_webpage",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619773845835344,
      "end_time": 1755619774253607141,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 3227612309571849201,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "execute_tool",
        "gen_ai.tool.name": "visit_webpage",
        "gen_ai.tool.description": "Visits a webpage at the given url and reads its content as a markdown string. Use this to browse webpages.",
        "gen_ai.tool.args": "{\"url\":\"https://blog.mozilla.ai/introducing-any-llm-a-unified-api-to-access-any-llm-provider/\",\"timeout\":30,\"max_length\":20000}",
        "gen_ai.output": "Introducing any-llm: A unified API to access any LLM provider\n[![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)](https://blog.mozilla.ai)\n[Sign in](#/portal/signin)\n[Subscribe](#/portal/signup)\nIntroducing any-llm: A unified API to access any LLM provider\n=============================================================\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers.\n[![Nathan Brake](/content/images/size/w160/2025/02/Nathan.png)](/author/nathan/)\n#### [Nathan Brake](/author/nathan/)\nJul 22, 2025\n\u2014 3 min read\n![Introducing any-llm: A unified API to access any LLM provider](/content/images/size/w1200/2025/07/any-llm.jpeg)\nTom Seidmann-Freud / [\u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale](https://pdimagearchive.org/images/bfb9c099-b4f3-41b2-8b4a-99773d86d7ba/)\nWhen it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As [we\u2019ve written about previously](https://blog.mozilla.ai/running-an-open-source-llm-in-2025/), there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\nToday, we\u2019re happy to announce the release of our new Python library: [any-llm](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai)! any-llm provides a simple unified interface to access the most popular providers. By changing only a [single configuration parameter](https://mozilla-ai.github.io/any-llm/quickstart/?ref=blog.mozilla.ai#basic-usage), you can easily switch between providers and models.\n```\nfrom any_llm import completion\nimport os\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n# Basic completion\nresponse = completion(\n    model=\"mistral/mistral-small-latest\", # <provider_id>/<model_id>\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n```\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\n1. **Use of provider SDKs**: any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\n2. **Committed to active maintenance**: any-llm is integrated with [any-agent](https://github.com/mozilla-ai/any-agent?ref=blog.mozilla.ai), one of our most community-engaged projects, so we're motivated to maintain it.\n3. **No proxy or gateway server required**: no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\nYou can view the list of our supported providers [here](https://mozilla-ai.github.io/any-llm/providers/?ref=blog.mozilla.ai).\nOpenAI API Standard\n-------------------\nThe [OpenAI API](https://openai.com/index/openai-api/?ref=blog.mozilla.ai) has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\nYou can view the any-llm API documentation [here](https://mozilla-ai.github.io/any-llm/api/completion/?ref=blog.mozilla.ai). And to see how it relates to existing solutions, read on.\nShortcomings of Existing Solutions\n----------------------------------\nSeveral excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\nOne popular solution, [LiteLLM](https://github.com/BerriAI/litellm?ref=blog.mozilla.ai), is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\nAnother option, [AISuite](https://github.com/andrewyng/aisuite/issues?ref=blog.mozilla.ai), was created by [Andrew NG](https://www.andrewng.org/?ref=blog.mozilla.ai) and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\nFurthermore, many framework-specific solutions, such as those found in [Agno](https://github.com/agno-agi/agno/tree/main/libs/agno/agno/models?ref=blog.mozilla.ai), either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\nLastly, proxy/gateway solutions like [OpenRouter](https://openrouter.ai/?ref=blog.mozilla.ai) and [Portkey](https://github.com/Portkey-AI/portkey-python-sdk?ref=blog.mozilla.ai) require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\n\ud83d\udca1\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our [repo](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai) and [documentation](https://mozilla-ai.github.io/any-llm/?ref=blog.mozilla.ai) and give it a try today!\nRead more\n---------\n[![Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm](/content/images/size/w600/2025/08/gpt-oss.jpg)\n### Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm\nAfter OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\nBy Nathan Brake\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\n[![Wasm-agents: AI agents running in your browser](/content/images/size/w600/2025/07/WASM-Agents.jpeg)\n### Wasm-agents: AI agents running in your browser\nOne of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\nBy Davide Eynard\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\n[![Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs](/content/images/size/w600/2025/05/RP-P-OB-6835.jpg)\n### Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs\nGenerative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\n[![The Challenge of Choosing the Right LLM](/content/images/size/w600/2025/05/Charles-Darwin.jpeg)\n### The Challenge of Choosing the Right LLM\nImagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/)\n![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)\n* [mozilla.ai](https://www.mozilla.ai/)\nPowered by [Ghost](https://ghost.org/)\nMozilla.ai\n----------\nSubscribe to receive updates\n[Subscribe](#/portal/signup)",
        "gen_ai.output.type": "text"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "call_llm o3",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619774255494711,
      "end_time": 1755619813474931962,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 210105378043717371,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "call_llm",
        "gen_ai.request.model": "o3",
        "gen_ai.output": "[{\"tool.name\": \"extract_text_from_markdown_or_html\", \"tool.args\": \"{\\\"content\\\":\\\"Introducing any-llm: A unified API to access any LLM provider\\\\n[![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)](https://blog.mozilla.ai)\\\\n[Sign in](#/portal/signin)\\\\n[Subscribe](#/portal/signup)\\\\nIntroducing any-llm: A unified API to access any LLM provider\\\\n=============================================================\\\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers.\\\\n[![Nathan Brake](/content/images/size/w160/2025/02/Nathan.png)](/author/nathan/)\\\\n#### [Nathan Brake](/author/nathan/)\\\\nJul 22, 2025\\\\n\u2014 3 min read\\\\n![Introducing any-llm: A unified API to access any LLM provider](/content/images/size/w1200/2025/07/any-llm.jpeg)\\\\nTom Seidmann-Freud / [\u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale](https://pdimagearchive.org/images/bfb9c099-b4f3-41b2-8b4a-99773d86d7ba/)\\\\nWhen it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As [we\u2019ve written about previously](https://blog.mozilla.ai/running-an-open-source-llm-in-2025/), there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\\\nToday, we\u2019re happy to announce the release of our new Python library: [any-llm](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai)! any-llm provides a simple unified interface to access the most popular providers. By changing only a [single configuration parameter](https://mozilla-ai.github.io/any-llm/quickstart/?ref=blog.mozilla.ai#basic-usage), you can easily switch between providers and models.\\\\n```\\\\nfrom any_llm import completion\\\\nimport os\\\\n# Make sure you have the appropriate environment variable set\\\\nassert os.environ.get('MISTRAL_API_KEY')\\\\n# Basic completion\\\\nresponse = completion(\\\\n    model=\\\\\\\"mistral/mistral-small-latest\\\\\\\", # <provider_id>/<model_id>\\\\n    messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello!\\\\\\\"}]\\\\n)\\\\nprint(response.choices[0].message.content)\\\\n```\\\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\\\n1. **Use of provider SDKs**: any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\\\n2. **Committed to active maintenance**: any-llm is integrated with [any-agent](https://github.com/mozilla-ai/any-agent?ref=blog.mozilla.ai), one of our most community-engaged projects, so we're motivated to maintain it.\\\\n3. **No proxy or gateway server required**: no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\\\nYou can view the list of our supported providers [here](https://mozilla-ai.github.io/any-llm/providers/?ref=blog.mozilla.ai).\\\\nOpenAI API Standard\\\\n-------------------\\\\nThe [OpenAI API](https://openai.com/index/openai-api/?ref=blog.mozilla.ai) has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\\\nYou can view the any-llm API documentation [here](https://mozilla-ai.github.io/any-llm/api/completion/?ref=blog.mozilla.ai). And to see how it relates to existing solutions, read on.\\\\nShortcomings of Existing Solutions\\\\n----------------------------------\\\\nSeveral excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\\\nOne popular solution, [LiteLLM](https://github.com/BerriAI/litellm?ref=blog.mozilla.ai), is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\\\nAnother option, [AISuite](https://github.com/andrewyng/aisuite/issues?ref=blog.mozilla.ai), was created by [Andrew NG](https://www.andrewng.org/?ref=blog.mozilla.ai) and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\\\nFurthermore, many framework-specific solutions, such as those found in [Agno](https://github.com/agno-agi/agno/tree/main/libs/agno/agno/models?ref=blog.mozilla.ai), either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\\\nLastly, proxy/gateway solutions like [OpenRouter](https://openrouter.ai/?ref=blog.mozilla.ai) and [Portkey](https://github.com/Portkey-AI/portkey-python-sdk?ref=blog.mozilla.ai) require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\\\n\ud83d\udca1\\\\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our [repo](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai) and [documentation](https://mozilla-ai.github.io/any-llm/?ref=blog.mozilla.ai) and give it a try today!\\\\nRead more\\\\n---------\\\\n[![Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm](/content/images/size/w600/2025/08/gpt-oss.jpg)\\\\n### Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm\\\\nAfter OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\\\\nBy Nathan Brake\\\\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\\\\n[![Wasm-agents: AI agents running in your browser](/content/images/size/w600/2025/07/WASM-Agents.jpeg)\\\\n### Wasm-agents: AI agents running in your browser\\\\nOne of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\\\\nBy Davide Eynard\\\\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\\\\n[![Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs](/content/images/size/w600/2025/05/RP-P-OB-6835.jpg)\\\\n### Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs\\\\nGenerative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\\\\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\\\\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\\\\n[![The Challenge of Choosing the Right LLM](/content/images/size/w600/2025/05/Charles-Darwin.jpeg)\\\\n### The Challenge of Choosing the Right LLM\\\\nImagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\\\\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\\\\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/)\\\\n![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)\\\\n* [mozilla.ai](https://www.mozilla.ai/)\\\\nPowered by [Ghost](https://ghost.org/)\\\\nMozilla.ai\\\\n----------\\\\nSubscribe to receive updates\\\\n[Subscribe](#/portal/signup)\\\",\\\"content_type\\\":\\\"md\\\"}\"}]",
        "gen_ai.output.type": "json",
        "gen_ai.usage.input_tokens": 3170,
        "gen_ai.usage.output_tokens": 2352,
        "gen_ai.usage.input_cost": 0.006339999999999999,
        "gen_ai.usage.output_cost": 0.018816
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "execute_tool extract_text_from_markdown_or_html",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619813487800533,
      "end_time": 1755619813565600713,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 16522645325077703608,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "execute_tool",
        "gen_ai.tool.name": "extract_text_from_markdown_or_html",
        "gen_ai.tool.description": "Preprocesses raw input content (Markdown or HTML) to extract plain text.",
        "gen_ai.tool.args": "{\"content\":\"Introducing any-llm: A unified API to access any LLM provider\\n[![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)](https://blog.mozilla.ai)\\n[Sign in](#/portal/signin)\\n[Subscribe](#/portal/signup)\\nIntroducing any-llm: A unified API to access any LLM provider\\n=============================================================\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers.\\n[![Nathan Brake](/content/images/size/w160/2025/02/Nathan.png)](/author/nathan/)\\n#### [Nathan Brake](/author/nathan/)\\nJul 22, 2025\\n\u2014 3 min read\\n![Introducing any-llm: A unified API to access any LLM provider](/content/images/size/w1200/2025/07/any-llm.jpeg)\\nTom Seidmann-Freud / [\u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale](https://pdimagearchive.org/images/bfb9c099-b4f3-41b2-8b4a-99773d86d7ba/)\\nWhen it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As [we\u2019ve written about previously](https://blog.mozilla.ai/running-an-open-source-llm-in-2025/), there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\nToday, we\u2019re happy to announce the release of our new Python library: [any-llm](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai)! any-llm provides a simple unified interface to access the most popular providers. By changing only a [single configuration parameter](https://mozilla-ai.github.io/any-llm/quickstart/?ref=blog.mozilla.ai#basic-usage), you can easily switch between providers and models.\\n```\\nfrom any_llm import completion\\nimport os\\n# Make sure you have the appropriate environment variable set\\nassert os.environ.get('MISTRAL_API_KEY')\\n# Basic completion\\nresponse = completion(\\n    model=\\\"mistral/mistral-small-latest\\\", # <provider_id>/<model_id>\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]\\n)\\nprint(response.choices[0].message.content)\\n```\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\n1. **Use of provider SDKs**: any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\n2. **Committed to active maintenance**: any-llm is integrated with [any-agent](https://github.com/mozilla-ai/any-agent?ref=blog.mozilla.ai), one of our most community-engaged projects, so we're motivated to maintain it.\\n3. **No proxy or gateway server required**: no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\nYou can view the list of our supported providers [here](https://mozilla-ai.github.io/any-llm/providers/?ref=blog.mozilla.ai).\\nOpenAI API Standard\\n-------------------\\nThe [OpenAI API](https://openai.com/index/openai-api/?ref=blog.mozilla.ai) has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\nYou can view the any-llm API documentation [here](https://mozilla-ai.github.io/any-llm/api/completion/?ref=blog.mozilla.ai). And to see how it relates to existing solutions, read on.\\nShortcomings of Existing Solutions\\n----------------------------------\\nSeveral excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\nOne popular solution, [LiteLLM](https://github.com/BerriAI/litellm?ref=blog.mozilla.ai), is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\nAnother option, [AISuite](https://github.com/andrewyng/aisuite/issues?ref=blog.mozilla.ai), was created by [Andrew NG](https://www.andrewng.org/?ref=blog.mozilla.ai) and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\nFurthermore, many framework-specific solutions, such as those found in [Agno](https://github.com/agno-agi/agno/tree/main/libs/agno/agno/models?ref=blog.mozilla.ai), either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\nLastly, proxy/gateway solutions like [OpenRouter](https://openrouter.ai/?ref=blog.mozilla.ai) and [Portkey](https://github.com/Portkey-AI/portkey-python-sdk?ref=blog.mozilla.ai) require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\n\ud83d\udca1\\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our [repo](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai) and [documentation](https://mozilla-ai.github.io/any-llm/?ref=blog.mozilla.ai) and give it a try today!\\nRead more\\n---------\\n[![Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm](/content/images/size/w600/2025/08/gpt-oss.jpg)\\n### Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm\\nAfter OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\\nBy Nathan Brake\\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\\n[![Wasm-agents: AI agents running in your browser](/content/images/size/w600/2025/07/WASM-Agents.jpeg)\\n### Wasm-agents: AI agents running in your browser\\nOne of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\\nBy Davide Eynard\\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\\n[![Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs](/content/images/size/w600/2025/05/RP-P-OB-6835.jpg)\\n### Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs\\nGenerative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\\n[![The Challenge of Choosing the Right LLM](/content/images/size/w600/2025/05/Charles-Darwin.jpeg)\\n### The Challenge of Choosing the Right LLM\\nImagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/)\\n![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)\\n* [mozilla.ai](https://www.mozilla.ai/)\\nPowered by [Ghost](https://ghost.org/)\\nMozilla.ai\\n----------\\nSubscribe to receive updates\\n[Subscribe](#/portal/signup)\",\"content_type\":\"md\"}",
        "gen_ai.output": "Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\n=============================================================\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\n```\nfrom any_llm import completion\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\n    model=\"mistral/mistral-small-latest\", # / messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n```\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\nYou can view the list of our supported providers here .\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\n\ud83d\udca1\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\nRead more [ Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm After OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\nBy Nathan Brake\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\n[ Wasm-agents: AI agents running in your browser One of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\nBy Davide Eynard\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\n[ Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs Generative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\n[ The Challenge of Choosing the Right LLM Imagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/) * mozilla.ai Powered by Ghost Mozilla.ai Subscribe to receive updates Subscribe",
        "gen_ai.output.type": "text"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "execute_tool extract_text_from_markdown_or_html",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619813487800533,
      "end_time": 1755619813565600713,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 16522645325077703608,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "execute_tool",
        "gen_ai.tool.name": "extract_text_from_markdown_or_html",
        "gen_ai.tool.description": "Preprocesses raw input content (Markdown or HTML) to extract plain text.",
        "gen_ai.tool.args": "{\"content\":\"Introducing any-llm: A unified API to access any LLM provider\\n[![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)](https://blog.mozilla.ai)\\n[Sign in](#/portal/signin)\\n[Subscribe](#/portal/signup)\\nIntroducing any-llm: A unified API to access any LLM provider\\n=============================================================\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers.\\n[![Nathan Brake](/content/images/size/w160/2025/02/Nathan.png)](/author/nathan/)\\n#### [Nathan Brake](/author/nathan/)\\nJul 22, 2025\\n\u2014 3 min read\\n![Introducing any-llm: A unified API to access any LLM provider](/content/images/size/w1200/2025/07/any-llm.jpeg)\\nTom Seidmann-Freud / [\u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale](https://pdimagearchive.org/images/bfb9c099-b4f3-41b2-8b4a-99773d86d7ba/)\\nWhen it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As [we\u2019ve written about previously](https://blog.mozilla.ai/running-an-open-source-llm-in-2025/), there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\nToday, we\u2019re happy to announce the release of our new Python library: [any-llm](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai)! any-llm provides a simple unified interface to access the most popular providers. By changing only a [single configuration parameter](https://mozilla-ai.github.io/any-llm/quickstart/?ref=blog.mozilla.ai#basic-usage), you can easily switch between providers and models.\\n```\\nfrom any_llm import completion\\nimport os\\n# Make sure you have the appropriate environment variable set\\nassert os.environ.get('MISTRAL_API_KEY')\\n# Basic completion\\nresponse = completion(\\n    model=\\\"mistral/mistral-small-latest\\\", # <provider_id>/<model_id>\\n    messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]\\n)\\nprint(response.choices[0].message.content)\\n```\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\n1. **Use of provider SDKs**: any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\n2. **Committed to active maintenance**: any-llm is integrated with [any-agent](https://github.com/mozilla-ai/any-agent?ref=blog.mozilla.ai), one of our most community-engaged projects, so we're motivated to maintain it.\\n3. **No proxy or gateway server required**: no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\nYou can view the list of our supported providers [here](https://mozilla-ai.github.io/any-llm/providers/?ref=blog.mozilla.ai).\\nOpenAI API Standard\\n-------------------\\nThe [OpenAI API](https://openai.com/index/openai-api/?ref=blog.mozilla.ai) has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\nYou can view the any-llm API documentation [here](https://mozilla-ai.github.io/any-llm/api/completion/?ref=blog.mozilla.ai). And to see how it relates to existing solutions, read on.\\nShortcomings of Existing Solutions\\n----------------------------------\\nSeveral excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\nOne popular solution, [LiteLLM](https://github.com/BerriAI/litellm?ref=blog.mozilla.ai), is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\nAnother option, [AISuite](https://github.com/andrewyng/aisuite/issues?ref=blog.mozilla.ai), was created by [Andrew NG](https://www.andrewng.org/?ref=blog.mozilla.ai) and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\nFurthermore, many framework-specific solutions, such as those found in [Agno](https://github.com/agno-agi/agno/tree/main/libs/agno/agno/models?ref=blog.mozilla.ai), either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\nLastly, proxy/gateway solutions like [OpenRouter](https://openrouter.ai/?ref=blog.mozilla.ai) and [Portkey](https://github.com/Portkey-AI/portkey-python-sdk?ref=blog.mozilla.ai) require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\n\ud83d\udca1\\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our [repo](https://github.com/mozilla-ai/any-llm?ref=blog.mozilla.ai) and [documentation](https://mozilla-ai.github.io/any-llm/?ref=blog.mozilla.ai) and give it a try today!\\nRead more\\n---------\\n[![Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm](/content/images/size/w600/2025/08/gpt-oss.jpg)\\n### Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm\\nAfter OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\\nBy Nathan Brake\\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\\n[![Wasm-agents: AI agents running in your browser](/content/images/size/w600/2025/07/WASM-Agents.jpeg)\\n### Wasm-agents: AI agents running in your browser\\nOne of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\\nBy Davide Eynard\\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\\n[![Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs](/content/images/size/w600/2025/05/RP-P-OB-6835.jpg)\\n### Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs\\nGenerative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\\n[![The Challenge of Choosing the Right LLM](/content/images/size/w600/2025/05/Charles-Darwin.jpeg)\\n### The Challenge of Choosing the Right LLM\\nImagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/)\\n![Mozilla.ai](https://blog.mozilla.ai/content/images/2025/01/mozilla.white-1.png)\\n* [mozilla.ai](https://www.mozilla.ai/)\\nPowered by [Ghost](https://ghost.org/)\\nMozilla.ai\\n----------\\nSubscribe to receive updates\\n[Subscribe](#/portal/signup)\",\"content_type\":\"md\"}",
        "gen_ai.output": "Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\n=============================================================\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\n```\nfrom any_llm import completion\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\n    model=\"mistral/mistral-small-latest\", # / messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n```\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\nYou can view the list of our supported providers here .\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\n\ud83d\udca1\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\nRead more [ Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm After OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\nBy Nathan Brake\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\n[ Wasm-agents: AI agents running in your browser One of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\nBy Davide Eynard\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\n[ Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs Generative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\n[ The Challenge of Choosing the Right LLM Imagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/) * mozilla.ai Powered by Ghost Mozilla.ai Subscribe to receive updates Subscribe",
        "gen_ai.output.type": "text"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "call_llm o3",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619813588419510,
      "end_time": 1755619836935745647,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 12572652201242876127,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "call_llm",
        "gen_ai.request.model": "o3",
        "gen_ai.output": "[{\"tool.name\": \"summarize_text_with_llm\", \"tool.args\": \"{\\\"text\\\":\\\"Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\\\\n=============================================================\\\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\\\\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\\\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\\\\n```\\\\nfrom any_llm import completion\\\\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\\\\n    model=\\\\\\\"mistral/mistral-small-latest\\\\\\\", # / messages=[{\\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello!\\\\\\\"}]\\\\n)\\\\nprint(response.choices[0].message.content)\\\\n```\\\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\\\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\\\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\\\\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\\\nYou can view the list of our supported providers here .\\\\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\\\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\\\\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\\\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\\\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\\\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\\\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\\\n\ud83d\udca1 We hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\\\",\\\"summary_length\\\":\\\"a concise paragraph\\\"}\"}]",
        "gen_ai.output.type": "json",
        "gen_ai.usage.input_tokens": 7099,
        "gen_ai.usage.output_tokens": 1137,
        "gen_ai.usage.input_cost": 0.014197999999999999,
        "gen_ai.usage.output_cost": 0.009096
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "execute_tool summarize_text_with_llm",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619836943225189,
      "end_time": 1755619840266470651,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 14325730297279971576,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "execute_tool",
        "gen_ai.tool.name": "summarize_text_with_llm",
        "gen_ai.tool.description": "Summarizes a given text using an LLM.",
        "gen_ai.tool.args": "{\"text\":\"Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\\n=============================================================\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\\n```\\nfrom any_llm import completion\\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\\n    model=\\\"mistral/mistral-small-latest\\\", # / messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]\\n)\\nprint(response.choices[0].message.content)\\n```\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\nYou can view the list of our supported providers here .\\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\n\ud83d\udca1 We hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\",\"summary_length\":\"a concise paragraph\"}",
        "gen_ai.output": "The any-llm is a newly released Python library that offers a unified API for accessing various Large Language Model (LLM) providers, allowing users to switch between models and providers with minimal configuration change. Its design leverages official SDKs from providers, ensuring compatibility and ease of use without the need for additional proxy services. By normalizing outputs to return consistent OpenAI ChatCompletion objects, any-llm addresses the compatibility issues faced by existing libraries. It aims to fill gaps left by current solutions that are either not actively maintained or impose complexities through additional layers. As such, any-llm is positioned as a versatile and user-friendly implementation for LLM application development.",
        "gen_ai.output.type": "text"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "execute_tool summarize_text_with_llm",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619836943225189,
      "end_time": 1755619840266470651,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 14325730297279971576,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "execute_tool",
        "gen_ai.tool.name": "summarize_text_with_llm",
        "gen_ai.tool.description": "Summarizes a given text using an LLM.",
        "gen_ai.tool.args": "{\"text\":\"Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\\n=============================================================\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\\n```\\nfrom any_llm import completion\\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\\n    model=\\\"mistral/mistral-small-latest\\\", # / messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]\\n)\\nprint(response.choices[0].message.content)\\n```\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\nYou can view the list of our supported providers here .\\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\n\ud83d\udca1 We hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\",\"summary_length\":\"a concise paragraph\"}",
        "gen_ai.output": "The any-llm is a newly released Python library that offers a unified API for accessing various Large Language Model (LLM) providers, allowing users to switch between models and providers with minimal configuration change. Its design leverages official SDKs from providers, ensuring compatibility and ease of use without the need for additional proxy services. By normalizing outputs to return consistent OpenAI ChatCompletion objects, any-llm addresses the compatibility issues faced by existing libraries. It aims to fill gaps left by current solutions that are either not actively maintained or impose complexities through additional layers. As such, any-llm is positioned as a versatile and user-friendly implementation for LLM application development.",
        "gen_ai.output.type": "text"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "execute_tool summarize_text_with_llm",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619836943225189,
      "end_time": 1755619840266470651,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 14325730297279971576,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "execute_tool",
        "gen_ai.tool.name": "summarize_text_with_llm",
        "gen_ai.tool.description": "Summarizes a given text using an LLM.",
        "gen_ai.tool.args": "{\"text\":\"Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\\n=============================================================\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\\n```\\nfrom any_llm import completion\\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\\n    model=\\\"mistral/mistral-small-latest\\\", # / messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]\\n)\\nprint(response.choices[0].message.content)\\n```\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\nYou can view the list of our supported providers here .\\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\n\ud83d\udca1 We hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\",\"summary_length\":\"a concise paragraph\"}",
        "gen_ai.output": "The any-llm is a newly released Python library that offers a unified API for accessing various Large Language Model (LLM) providers, allowing users to switch between models and providers with minimal configuration change. Its design leverages official SDKs from providers, ensuring compatibility and ease of use without the need for additional proxy services. By normalizing outputs to return consistent OpenAI ChatCompletion objects, any-llm addresses the compatibility issues faced by existing libraries. It aims to fill gaps left by current solutions that are either not actively maintained or impose complexities through additional layers. As such, any-llm is positioned as a versatile and user-friendly implementation for LLM application development.",
        "gen_ai.output.type": "text"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "call_llm o3",
      "kind": "internal",
      "parent": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619840328564490,
      "end_time": 1755619886229517424,
      "status": {
        "status_code": "ok",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 14814899904699744835,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "call_llm",
        "gen_ai.request.model": "o3",
        "gen_ai.output": "{\"url\":\"https://blog.mozilla.ai/introducing-any-llm-a-unified-api-to-access-any-llm-provider/\",\"extracted_text\":\"Introducing any-llm: A unified API to access any LLM provider Sign in Subscribe Introducing any-llm: A unified API to access any LLM provider\\n=============================================================\\nWhen it comes to using LLMs, it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. Today, we announce the release of any-llm, a Python library that provides a simple unified interface to access the most popular providers. Nathan Brake Jul 22, 2025\\n\u2014 3 min read Tom Seidmann-Freud / \u201cWhy the Hare Has No Tail\u201d, a Zulu fairy tale When it comes to using Large Language Models (LLMs), it\u2019s not always a question of which model to use: it\u2019s also a matter of choosing who provides the LLM and where it is deployed. As we\u2019ve written about previously , there are many options available for how to access an LLM. The provider you choose to use can have implications in terms of cost, latency, and security. Most AI labs offer their own provider platform (OpenAI, Google, Mistral, etc.), and other provider platforms (Azure, AWS, Cerebras, etc.) provide access to a wide variety of LLMs. But what if you want to build your LLM application without having to worry about being locked in to a certain provider?\\nToday, we\u2019re happy to announce the release of our new Python library: any-llm ! any-llm provides a simple unified interface to access the most popular providers. By changing only a single configuration parameter , you can easily switch between providers and models.\\n```\\nfrom any_llm import completion\\nimport os Make sure you have the appropriate environment variable set assert os.environ.get('MISTRAL_API_KEY') Basic completion response = completion(\\n    model=\\\"mistral/mistral-small-latest\\\", # / messages=[{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]\\n)\\nprint(response.choices[0].message.content)\\n```\\nany-llm fills a gap in the LLM provider interface landscape through several key design principles:\\n1. Use of provider SDKs : any-llm leverages official provider SDKs when available, reducing the maintenance burden and ensuring compatibility.\\n2. Committed to active maintenance : any-llm is integrated with any-agent , one of our most community-engaged projects, so we're motivated to maintain it.\\n3. No proxy or gateway server required : no need to set up another service as a gateway: download the any-llm SDK, and you\u2019re good to communicate with all the supported providers, without having to send your data through another third-party provider.\\nYou can view the list of our supported providers here .\\nOpenAI API Standard The OpenAI API has become the standard for LLM provider interfaces. Although some providers provide perfect compatibility with the OpenAI API standard, others (like Mistral, Anthropic, etc.) may slightly diverge from the OpenAI standard when it comes to expected input parameters and output values.\\nIn order to make it easy to switch between these providers, this creates a need for lightweight wrappers that can gracefully handle these differences while maintaining as consistent an interface as possible.\\nany-llm solves this problem by normalizing outputs to return OpenAI ChatCompletion objects, regardless of which provider is used under the hood. The objects are returned as OpenAI Pydantic models, so you can access them just as you would if you were using the official OpenAI API SDK.\\nYou can view the any-llm API documentation here . And to see how it relates to existing solutions, read on.\\nShortcomings of Existing Solutions Several excellent open-source Python libraries already exist to address the challenge of interacting with various LLM providers, but each comes with its own set of limitations.\\nOne popular solution, LiteLLM , is highly valued for its wide support of different providers and modalities, making it a great choice for many developers. However, it re-implements provider interfaces rather than leveraging SDKs that are managed and released by the providers themselves. As a result, the approach can lead to compatibility issues and unexpected modifications in behavior, making it difficult to keep up with the changes happening among all the providers.\\nAnother option, AISuite , was created by Andrew NG and offers a clean and modular design. However, it is not actively maintained (its last release was in December of 2024) and lacks consistent Python-typed interfaces.\\nFurthermore, many framework-specific solutions, such as those found in Agno , either depend on LiteLLM or implement their own provider integrations. These implementations may be robust, but are difficult to integrate into other software solutions since they\u2019re tightly coupled with the software already built around them.\\nLastly, proxy/gateway solutions like OpenRouter and Portkey require users to set up a hosted proxy server to act as an intermediary between their code and the LLM provider. Although this can effectively abstract away the complicated logic from the developer, it adds an extra layer of complexity and a dependency on external services, which might not be ideal for all use cases.\\n\ud83d\udca1\\nWe hope that any-llm will prove to be a useful alternative to the already available solutions. Take a look at our repo and documentation and give it a try today!\\nRead more [ Standardized Reasoning Content: A first look at using OpenAI\u2019s gpt-oss on multiple providers using any-llm After OpenAI\u2019s ChatGPT release, the default standard for communication was the OpenAI Completion API. However, with the new \u201creasoning models\u201d, a critical piece of the output isn\u2019t handled by that OpenAI specification, leaving each provider to decide how to handle the new model capabilities.\\nBy Nathan Brake\\nAug 12, 2025](/standardized-reasoning-content-a-first-look-at-using-openais-gpt-oss-on-multiple-providers-using-any-llm/)\\n[ Wasm-agents: AI agents running in your browser One of the main barriers to a wider adoption of open-source agents is the dependency on extra tools and frameworks that need to be installed before the agents can be run. In this post, we show how to write agents as HTML files, which can just be opened and run in a browser.\\nBy Davide Eynard\\nJul 3, 2025](/wasm-agents-ai-agents-running-in-your-browser/)\\n[ Smarter Prompts for Better Responses: Exploring Prompt Optimization and Interpretability for LLMs Generative AI models are highly sensitive to input phrasing. Even small changes to a prompt or switching between models can lead to different results. Adding to the complexity, LLMs often act as black-boxes, making it difficult to understand how specific prompts influence their behavior.\\nBy Davide Eynard, Hareesh Bahuleyan, Sandra Antunes\\nMay 14, 2025](/smarter-prompts-for-better-responses-exploring-prompt-optimization-and-interpretability-for-llms/)\\n[ The Challenge of Choosing the Right LLM Imagine you could effortlessly navigate the universe of LLMs, always knowing which one is the perfect fit for your specific query. Today, this is a very difficult challenge. So, how do you efficiently manage and use LLMs for various tasks? This is where LLM Routing emerges as a crucial strategy.\\nBy Dimitris Poulopoulos, Mario David Cari\u00f1ana Abasolo, Juliana Rodrigues Ara\u00fajo\\nMay 8, 2025](/the-challenge-of-choosing-the-right-llm/) * mozilla.ai Powered by Ghost Mozilla.ai Subscribe to receive updates Subscribe\",\"summary\":\"Mozilla.ai announces \u201cany-llm,\u201d a Python library that lets developers access multiple Large Language Model providers through a single, interchangeable API. By specifying one configuration value, users can swap models and providers while receiving standardized OpenAI-style ChatCompletion objects. any-llm relies on providers\u2019 official SDKs, is actively maintained via the any-agent project, and works locally without proxy or gateway servers. It aims to resolve compatibility and maintenance issues seen in existing libraries such as LiteLLM, AISuite, framework-specific wrappers, and proxy solutions like OpenRouter or Portkey, offering a simpler, vendor-agnostic path for LLM application development.\"}",
        "gen_ai.output.type": "json",
        "gen_ai.usage.input_tokens": 8387,
        "gen_ai.usage.output_tokens": 1791,
        "gen_ai.usage.input_cost": 0.016774,
        "gen_ai.usage.output_cost": 0.014327999999999999
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    },
    {
      "name": "invoke_agent [any_agent]",
      "kind": "internal",
      "parent": {
        "trace_id": null,
        "span_id": null,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "start_time": 1755619768394802103,
      "end_time": 1755619886236915703,
      "status": {
        "status_code": "unset",
        "description": null
      },
      "context": {
        "trace_id": 113373236198540926847564014454126543202,
        "span_id": 9983627894118722516,
        "is_remote": false,
        "trace_flags": {
          "value": 0
        },
        "trace_state": {
          "entries": {}
        }
      },
      "attributes": {
        "gen_ai.operation.name": "invoke_agent",
        "gen_ai.agent.name": "any_agent",
        "gen_ai.agent.description": "No description.",
        "gen_ai.request.model": "o3"
      },
      "links": [],
      "events": [],
      "resource": {
        "attributes": {
          "telemetry.sdk.language": "python",
          "telemetry.sdk.name": "opentelemetry",
          "telemetry.sdk.version": "1.36.0",
          "service.name": "unknown_service"
        },
        "schema_url": ""
      }
    }
  ],
  "final_output": {},
  "execution_costs": {
    "input_cost": 0.039026,
    "output_cost": 0.042696,
    "total_cost": 0.08172199999999999
  }
}
