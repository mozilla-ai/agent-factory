{
  "message": "✅ Done! Your agent is ready!",
  "status": "completed",
  "imports": "import os\nfrom any_agent.tools import search_tavily\nfrom tools.extract_text_from_url import extract_text_from_url\nfrom tools.summarize_text_with_llm import summarize_text_with_llm",
  "agent_instructions": "You are an assistant that summarizes the main textual content of a publicly accessible webpage.  Follow this step-by-step workflow meticulously:\n\nStep 1 – Input\nReceive exactly one webpage URL from the user.\n\nStep 2 – Extract\nCall the tool `extract_text_from_url` with the provided URL to fetch and extract the primary textual content of the page.  Focus on article or body text and automatically discard / ignore navigation menus, ads, comments, sidebars, footers, scripts, or any other non-core elements.\n\nStep 3 – Validate\nIf extraction fails or returns empty text, stop and set the `summary` field to a clear error explanation while leaving `extracted_text` empty.\n\nStep 4 – Summarize\nIf meaningful text was extracted, invoke `summarize_text_with_llm` and produce a concise, coherent summary of 5–8 sentences (≤ 200 words) capturing the key ideas.  The summary must be self-contained, faithful to the source, and avoid long verbatim quotations.\n\nStep 5 – Optional Web Search\nYou may optionally consult `search_tavily` if you need extra context or to clarify ambiguous references, but do not include external information that is not present in the original webpage content.\n\nStep 6 – Output\nReturn a JSON object that conforms exactly to the `StructuredOutput` schema with:\n• url – the original URL provided.\n• extracted_text – the raw extracted text, truncated to the first 1500 characters if longer.\n• summary – the summary from Step 4 or the error message from Step 3.\n\nGeneral Rules\n• Never invent or add facts not found in the page.\n• Do not output anything except the final JSON object.\n• Keep all tool calls explicit and arguments well-formed.",
  "tools": "TOOLS = [\n    extract_text_from_url,          # fetch & extract main text\n    summarize_text_with_llm,        # build concise summary\n    search_tavily,                  # optional web search if needed\n]",
  "structured_outputs": "class StructuredOutput(BaseModel):\n    \"\"\"Schema for the agent's final response.\"\"\"\n\n    url: str = Field(..., description=\"The original webpage URL provided by the user.\")\n    extracted_text: str = Field(\n        ..., description=\"The primary textual content extracted from the webpage. Truncated to 1500 characters if longer.\"\n    )\n    summary: str = Field(\n        ..., description=\"A concise summary (≤ 200 words) of the extracted text, or an error explanation if extraction failed.\"\n    )",
  "cli_args": "url: str",
  "agent_description": "Extract the main textual content from a webpage URL and return a concise summary in structured JSON format.",
  "prompt_template": "f\"Summarize the webpage at the URL: {url}\"",
  "readme": "# Webpage Summarizer Agent\n\nThis repository contains **agent.py**, an Any-Agent–powered script that extracts the primary text from any publicly accessible webpage and produces a concise summary.\n\n## Environment Variables\nCreate a `.env` file in the project root with the following keys:\n\n```ini\nOPENAI_API_KEY=<your OpenAI key>\nTAVILY_API_KEY=<your Tavily key>   # required only if the agent chooses to call search_tavily\n```\n\n## Install the Python package manager `uv`\n* **macOS / Linux**  \n  `curl -LsSf https://astral.sh/uv/install.sh | sh`\n* **Windows**  \n  `powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"`\n\n## Run the Agent\n```bash\nuv run --with-requirements requirements.txt --python 3.13 python agent.py --url \"https://example.com/article\"\n```\n\nThe agent will output a JSON object containing the original URL, the extracted text (truncated to 1500 characters), and a concise summary.",
  "dependencies": "any-agent[all,a2a]==0.25.0\npython-dotenv\nbeautifulsoup4\nrequests\nfire"
}
