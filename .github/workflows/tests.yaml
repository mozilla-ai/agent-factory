name: Tests

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'eval/**'
      - 'tests/**'
  pull_request:
    paths:
      - 'src/**'
      - 'eval/**'
      - 'tests/**'
  workflow_dispatch:

jobs:
  run-unit-tests:
    timeout-minutes: 10
    runs-on: ubuntu-latest

    steps:
      - name: Check out the repository
        uses: actions/checkout@v4

      - name: Install the latest version of uv and set the python version to 3.13
        uses: astral-sh/setup-uv@v6
        with:
          python-version: 3.13
          activate-environment: true

      - name: Cache mcpd binary
        id: cache-mcpd
        uses: actions/cache@0400d5f644dc74513175e3cd8d07132dd4860809 # v4.2.4
        with:
          key: mcpd-${{ vars.MCPD_VERSION }}-Linux-x86_64
          path: /usr/local/bin/mcpd

      - name: Install mcpd (if not cached)
        if: steps.cache-mcpd.outputs.cache-hit != 'true'
        run: |
          version="${{ vars.MCPD_VERSION }}"
          if [ -z "$version" ]; then
            echo "MCPD_VERSION var is not set, or empty"
            exit 1
          fi
          zip="mcpd_Linux_x86_64.tar.gz"
          url="https://github.com/mozilla-ai/mcpd/releases/download/${version}/${zip}"
          curl -fsSL "$url" -o "$zip" || { echo "download failed"; exit 1; }
          tar -xzf "$zip" mcpd || { echo "extract failed"; exit 1; }
          sudo mv mcpd /usr/local/bin/mcpd && sudo chmod +x /usr/local/bin/mcpd || { echo "Install failed"; return 1; }
          rm -f "$zip"

      - name: Run Agent Factory unit tests
        run: make test-unit


  run-generation-tests:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 30
    runs-on: ubuntu-latest
    needs: run-unit-tests
    strategy:
      fail-fast: false
      matrix:
        prompt_id:
          - summarize-url-content
          - url-to-podcast
          - scoring-blueprints-submission

    steps:
      - name: Check out the repository
        uses: actions/checkout@v4

      - name: Install the latest version of uv and set the python version to 3.13
        uses: astral-sh/setup-uv@v6
        with:
          python-version: 3.13
          activate-environment: true

      - name: Create .env file from GitHub secrets
        run: |
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" > .env

      - name: Run Generation Test - ${{ matrix.prompt_id }}
        run: make test-single-turn-generation-e2e PROMPT_ID=${{ matrix.prompt_id }}

  run-agent-evaluation-tests:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 10
    runs-on: ubuntu-latest
    needs: run-unit-tests

    steps:
      - name: Check out the repository
        uses: actions/checkout@v4

      - name: Install the latest version of uv and set the python version to 3.13
        uses: astral-sh/setup-uv@v6
        with:
          python-version: 3.13
          activate-environment: true

      - name: Install Python dependencies
        run: |
          uv sync --group tests

      - name: Run Generated Agent Evaluation tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        run: pytest -v tests/generated_agent_evaluation


  run-generated-artifacts-tests:
    timeout-minutes: 10
    runs-on: ubuntu-latest

    steps:
      - name: Check out the repository
        uses: actions/checkout@v4

      - name: Install the latest version of uv and set the python version to 3.13
        uses: astral-sh/setup-uv@v6
        with:
          python-version: 3.13
          activate-environment: true

      - name: Run Generated Artifacts tests
        run: |
          PROMPT_IDS=(
            summarize-url-content
            url-to-podcast
            scoring-blueprints-submission
          )
          for prompt_id in "${PROMPT_IDS[@]}"; do
            echo "::group::Running tests for PROMPT_ID: $prompt_id"
            make test-generated-artifacts PROMPT_ID="$prompt_id" || { echo "::error::Tests for PROMPT_ID $prompt_id failed!"; exit 1; }
            echo "::endgroup::"
          done

  run-generated-artifacts-integration-tests:
    if: github.event_name == 'workflow_dispatch'
    timeout-minutes: 30
    runs-on: ubuntu-latest

    steps:
      - name: Check out the repository
        uses: actions/checkout@v4

      - name: Install the latest version of uv and set the python version to 3.13
        uses: astral-sh/setup-uv@v6
        with:
          python-version: 3.13
          activate-environment: true

      - name: Run Generated Artifacts Integration tests
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          ELEVENLABS_API_KEY: "mock" # pragma: allowlist secret
        run: |
          PROMPT_IDS=()
          for prompt_id in "${PROMPT_IDS[@]}"; do
            echo "::group::Running tests for PROMPT_ID: $prompt_id"
            make test-generated-artifacts-integration PROMPT_ID="$prompt_id" || { echo "::error::Tests for PROMPT_ID $prompt_id failed!"; exit 1; }
            echo "::endgroup::"
          done
